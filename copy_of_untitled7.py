# -*- coding: utf-8 -*-
"""Copy of Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q1kaAZ3BFHoB7_lP1Bdq-Q7jALXbXPPe
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectFromModel
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# 1. Data Loading
try:
    df = pd.read_csv('housing.csv')
    print("Data loaded successfully from housing.csv")
    print(f"Dataset shape: {df.shape}")
    print("\nFirst 5 rows:")
    print(df.head())
except Exception as e:
    print(f"Error loading data: {e}")
    exit()

# 2. Data Preprocessing
def preprocess_data(df):
    # Make a copy of the dataframe
    data = df.copy()

    # Handle missing values - first check which columns have missing values
    print("\nMissing values before preprocessing:")
    print(data.isnull().sum()[data.isnull().sum() > 0])

    # Fill missing values for specific columns
    # For 'Lot Frontage' (note the space in column name)
    if 'Lot Frontage' in data.columns:
        data['Lot Frontage'].fillna(data['Lot Frontage'].median(), inplace=True)

    # For other numeric columns, fill with median
    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns
    for col in numeric_cols:
        if data[col].isnull().sum() > 0:
            data[col].fillna(data[col].median(), inplace=True)

    # For categorical columns, fill with mode
    categorical_cols = data.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if data[col].isnull().sum() > 0:
            data[col].fillna(data[col].mode()[0], inplace=True)

    # Remove outliers using IQR method for numerical features
    def remove_outliers(df, col):
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

    if 'SalePrice' in data.columns:
        outlier_cols = ['SalePrice', 'Gr Liv Area', 'Total Bsmt SF', 'Lot Area']
        for col in outlier_cols:
            if col in data.columns:
                data = remove_outliers(data, col)

    print("\nMissing values after preprocessing:")
    print(data.isnull().sum()[data.isnull().sum() > 0])

    return data

# Apply preprocessing
print("\nPreprocessing data...")
df_clean = preprocess_data(df)
print(f"\nData shape after preprocessing: {df_clean.shape}")

# 3. Exploratory Data Analysis (EDA)
def perform_eda(df):
    # Set style for plots
    sns.set(style="whitegrid")

    # 1. Univariate Analysis
    # Distribution of SalePrice
    plt.figure(figsize=(10, 6))
    sns.histplot(df['SalePrice'], kde=True, bins=30)
    plt.title('Distribution of House Prices')
    plt.xlabel('Sale Price')
    plt.ylabel('Frequency')
    plt.show()

    # Boxplot for OverallQual
    if 'Overall Qual' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.boxplot(x=df['Overall Qual'], y=df['SalePrice'])
        plt.title('House Prices by Overall Quality')
        plt.xlabel('Overall Quality Rating')
        plt.ylabel('Sale Price')
        plt.show()

    # 2. Bivariate Analysis
    # Scatter plot of GrLivArea vs SalePrice
    if 'Gr Liv Area' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x=df['Gr Liv Area'], y=df['SalePrice'])
        plt.title('Living Area vs Sale Price')
        plt.xlabel('Above Ground Living Area (sq ft)')
        plt.ylabel('Sale Price')
        plt.show()

    # Correlation heatmap
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    if len(numeric_cols) > 0:
        plt.figure(figsize=(16, 12))
        corr_matrix = df[numeric_cols].corr()
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm',
                    annot_kws={"size": 8}, vmin=-1, vmax=1)
        plt.title('Correlation Heatmap of Numerical Features')
        plt.show()

# Perform EDA
print("\nPerforming Exploratory Data Analysis...")
perform_eda(df_clean)

# 4. Feature Engineering
def feature_engineering(df):
    data = df.copy()

    # Create new features
    if 'Yr Sold' in data.columns and 'Year Built' in data.columns:
        data['HouseAge'] = data['Yr Sold'] - data['Year Built']

    if 'Full Bath' in data.columns and 'Half Bath' in data.columns:
        data['TotalBathrooms'] = data['Full Bath'] + (0.5 * data['Half Bath'])

    if 'Total Bsmt SF' in data.columns and '1st Flr SF' in data.columns and '2nd Flr SF' in data.columns:
        data['TotalSF'] = data['Total Bsmt SF'] + data['1st Flr SF'] + data['2nd Flr SF']

    # Log transformation of skewed features
    skewed_features = ['SalePrice', 'Gr Liv Area', 'Total Bsmt SF', 'Lot Area']
    for feature in skewed_features:
        if feature in data.columns and data[feature].skew() > 0.5:
            data[feature] = np.log1p(data[feature])

    return data

# Apply feature engineering
print("\nPerforming Feature Engineering...")
df_fe = feature_engineering(df_clean)
print("\nNew features created:")
print(df_fe[['HouseAge', 'TotalBathrooms', 'TotalSF']].head())

# 5. Prepare Data for Modeling
# Define features and target
X = df_fe.drop('SalePrice', axis=1)
y = df_fe['SalePrice']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify categorical and numerical columns
categorical_cols = X_train.select_dtypes(include=['object']).columns
numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns

# Create preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)])

# 6. Model Building and Evaluation
def build_and_evaluate_models(X_train, y_train, X_test, y_test, preprocessor):
    # Dictionary to store model performance
    model_performance = {}

    # List of models to evaluate
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(),
        'Lasso Regression': Lasso(),
        'Random Forest': RandomForestRegressor(random_state=42),
        'XGBoost': XGBRegressor(random_state=42),
        'Support Vector Regression': SVR()
    }

    # Parameter grids for hyperparameter tuning
    param_grids = {
        'Ridge Regression': {'model__alpha': [0.1, 1, 10, 100]},
        'Lasso Regression': {'model__alpha': [0.1, 1, 10, 100]},
        'Random Forest': {'model__n_estimators': [100, 200],
                         'model__max_depth': [None, 10, 20]},
        'XGBoost': {'model__n_estimators': [100, 200],
                    'model__learning_rate': [0.01, 0.1],
                    'model__max_depth': [3, 6]},
        'Support Vector Regression': {'model__C': [0.1, 1, 10],
                                     'model__kernel': ['linear', 'rbf']}
    }

    # Train and evaluate each model
    for model_name, model in models.items():
        print(f"\nTraining {model_name}...")

        # Create pipeline
        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('model', model)])

        # Hyperparameter tuning if parameters are specified
        if model_name in param_grids:
            grid_search = GridSearchCV(pipeline, param_grids[model_name],
                                     cv=5, scoring='neg_mean_squared_error')
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            print(f"Best parameters: {grid_search.best_params_}")
        else:
            best_model = pipeline
            best_model.fit(X_train, y_train)

        # Make predictions
        y_pred = best_model.predict(X_test)

        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        # Store performance
        model_performance[model_name] = {
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2
        }

        # Print metrics
        print(f"{model_name} Performance:")
        print(f"RMSE: {rmse:.4f}")
        print(f"MAE: {mae:.4f}")
        print(f"R2 Score: {r2:.4f}")

        # Cross-validation scores
        cv_scores = cross_val_score(best_model, X_train, y_train,
                                   cv=5, scoring='neg_mean_squared_error')
        cv_rmse = np.sqrt(-cv_scores.mean())
        print(f"Cross-validated RMSE: {cv_rmse:.4f}")

        # Feature importance for tree-based models
        if model_name in ['Random Forest', 'XGBoost']:
            try:
                # Get feature names after one-hot encoding
                if 'preprocessor' in best_model.named_steps:
                    preprocessor = best_model.named_steps['preprocessor']
                    feature_names = numeric_cols.tolist()

                    # Add one-hot encoded feature names
                    if 'cat' in preprocessor.named_transformers_:
                        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']
                        cat_features = ohe.get_feature_names_out(categorical_cols)
                        feature_names.extend(cat_features)

                    # Get feature importances
                    if model_name == 'Random Forest':
                        importances = best_model.named_steps['model'].feature_importances_
                    else:  # XGBoost
                        importances = best_model.named_steps['model'].feature_importances_

                    # Create feature importance dataframe
                    feature_importance = pd.DataFrame({
                        'Feature': feature_names,
                        'Importance': importances
                    }).sort_values('Importance', ascending=False)

                    # Plot top 10 features
                    plt.figure(figsize=(10, 6))
                    sns.barplot(x='Importance', y='Feature',
                                data=feature_importance.head(10))
                    plt.title(f'{model_name} - Top 10 Important Features')
                    plt.show()
            except Exception as e:
                print(f"Could not plot feature importance: {e}")

    return model_performance

# Build and evaluate models
print("\nBuilding and evaluating models...")
performance_results = build_and_evaluate_models(X_train, y_train, X_test, y_test, preprocessor)

# 7. Results Visualization
def visualize_results(y_test, y_pred, model_name):
    # Convert back from log scale if we transformed SalePrice
    if df_fe['SalePrice'].dtype == 'float64' and df_fe['SalePrice'].max() < 20:  # Assuming log transformed
        y_test = np.expm1(y_test)
        y_pred = np.expm1(y_pred)

    # Actual vs Predicted plot
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.xlabel('Actual Prices')
    plt.ylabel('Predicted Prices')
    plt.title(f'{model_name} - Actual vs Predicted Prices')
    plt.show()

    # Residual plot
    residuals = y_test - y_pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Prices')
    plt.ylabel('Residuals')
    plt.title(f'{model_name} - Residual Plot')
    plt.show()

    # Distribution plot
    plt.figure(figsize=(10, 6))
    sns.histplot(y_test, color='blue', label='Actual', kde=True, alpha=0.5)
    sns.histplot(y_pred, color='red', label='Predicted', kde=True, alpha=0.5)
    plt.title(f'{model_name} - Distribution of Actual vs Predicted Prices')
    plt.legend()
    plt.show()

# Example visualization for the best model (using XGBoost as example)
print("\nVisualizing results for XGBoost model...")
best_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', XGBRegressor(random_state=42, learning_rate=0.1, max_depth=3, n_estimators=200))])
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
visualize_results(y_test, y_pred, 'XGBoost')

# 8. Final Model Selection and Insights
# Compare all models
performance_df = pd.DataFrame.from_dict(performance_results, orient='index')
performance_df.sort_values(by='RMSE', ascending=True, inplace=True)
print("\nModel Performance Comparison:")
print(performance_df)

# Select the best model based on RMSE
best_model_name = performance_df.index[0]
print(f"\nBest performing model: {best_model_name}")

# Key insights
print("\nKey Insights:")
print("1. The most important features for house price prediction are typically:")
print("   - Overall quality of the house (Overall Qual)")
print("   - Above ground living area (Gr Liv Area)")
print("   - Total square footage (TotalSF - engineered feature)")
print("   - Neighborhood")
print("   - Number of bathrooms (TotalBathrooms - engineered feature)")
print("\n2. Tree-based models (Random Forest, XGBoost) generally perform better than linear models.")
print("\n3. Feature engineering (creating TotalSF, HouseAge) significantly improves model performance.")
print("\n4. Log transformation of skewed features helps normalize their distribution.")
print("\n5. The best model can be saved and deployed for making predictions on new data.")

# Save the best model
import joblib
best_model.fit(X, y)  # Retrain on full dataset
joblib.dump(best_model, 'best_house_price_model.pkl')
print("\nBest model saved as 'best_house_price_model.pkl'")